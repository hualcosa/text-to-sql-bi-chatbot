{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405fc0fe-553f-4d7b-9cdf-bdf098d48b0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text2SQL\n",
    "\n",
    "In this notebook we explore how to generate a valid SQL query using LLMs. Some key modules include extracting relevant context and best practices in prompt engineering, to achieve desired outcome. \n",
    "\n",
    "First we show an example that does not work to illustrate how important database schema context is for the model to generate valid SQL code.\n",
    "\n",
    "In the second example, the database schema is included in the prompt's context which results in a successful query execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb8694-92b0-4868-afb4-2ca308066338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00bc32-19b1-427d-82c7-765145e46a8f",
   "metadata": {},
   "source": [
    "## Bedrock Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7fa48-b149-49b3-9adf-bb01928cf7dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ../utilities/bedrock_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800efec4-f177-4e9d-a8de-f41a25443f3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## No context \n",
    "\n",
    "Here we demonstrate how important context is for the model to generate valid SQL queries. Without the database schema in the context, the model fails to generate valid SQL. \n",
    "\n",
    "Suggested questions to try:\n",
    "1. How many games were played each year?\n",
    "2. Give me a list of team names ordered by year it was founded.\n",
    "3. Which teams where founded before 1960s and active in 2000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e874b-e896-40c1-8882-fcab6466fbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ../utilities/database_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e8500-9834-4eaa-a882-f7a601579e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ../utilities/prompt_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905ae85-3486-42fd-b0a3-d00210b86be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(inspect.getsource(get_sql_query_prompt_no_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7a911-8982-4cc0-8d10-4df0da87d0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(inspect.getsource(get_sql_query_with_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eeb269-611c-4378-9981-a6fb99851590",
   "metadata": {},
   "source": [
    "One can see in this example what happens when context is not given. The model generates what appears to be valid SQL, but would actually fail when executing against the database.\n",
    "```python\n",
    "# Generate SQL Query with no context\n",
    "usr_query = \"How many games were played each year?\"\n",
    "system_prompt, usr_msg = get_sql_query_prompt_no_context(usr_query)\n",
    "sql_query = get_sql_query_with_llm(usr_msg, system_prompt , model_id=SONNET35_MODEL_ID)\n",
    "print(sql_query)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Expected Output\n",
    "\"\"\"SELECT YEAR(game_date) AS year, COUNT(*) AS games_played\n",
    "FROM games\n",
    "GROUP BY YEAR(game_date)\n",
    "ORDER BY year;\"\"\"\n",
    "```\n",
    "\n",
    "```python\n",
    "# Execute Query\n",
    "if_executable, response = execute_sql(sql_query)\n",
    "if not if_executable:\n",
    "    print(response['Error'])\n",
    "else:\n",
    "    print(response)\n",
    "```    \n",
    "\n",
    "```python\n",
    "# Expected Output\n",
    "\"\"\"EXECUTING..ERROR: relation \"games\" does not exist\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f86feb-34d6-479c-9de0-394131abb166",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With context\n",
    "\n",
    "Here we demonstrate providing context in the form of table definitions. The table definitions include the table name, list of column names for each table and the type for each column. \n",
    "\n",
    "Suggested questions to try:\n",
    "1. How many games were played each year?\n",
    "2. Give me a list of team names ordered by year it was founded.\n",
    "3. Which teams where founded before 1960s and active in 2000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc9454-a26e-4bc6-b117-69b7629256f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "schemas_dict = get_all_table_schema()\n",
    "db_schema = \"\\n\\n\".join(schemas_dict)\n",
    "print(db_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860bf45-558f-49b0-8fba-82237850948f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"db_schema.txt\", \"w\") as f:\n",
    "    f.write(db_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc8360-2632-4009-92d8-f8d6e5e30805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "usr_query = \"How many games were played each year?\"\n",
    "system_prompt, usr_msg = get_sql_query_prompt_generic_context(usr_query, db_schema)\n",
    "sql_query = get_sql_query_with_llm(usr_msg, system_prompt, model_id=SONNET35_MODEL_ID)\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf31a14-acee-4ae3-82da-e2ba0b8970e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if_executable, response = execute_sql(sql_query)\n",
    "if not if_executable:\n",
    "    print(response['Error'])\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277cf62-3bb2-445c-b4b4-0bf1e5ff2df4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Future extensions\n",
    "\n",
    "* Relevant context\n",
    "    * Challenge: when number of tables increases significantly, it might not fit in the context window or irrelevant tables may add noise.\n",
    "    * Solution: Do a pre-processing step with less detailed view of the database (e.g., table summary rather than definition) to identify relevant tables\n",
    "* Additional context\n",
    "    * Challenge-1: Table definitions may not be sufficient to understand the content in the table (e.g. value format, unique values for categorical)\n",
    "    * Solution-1: a) Provide few rows from each table as sample. b) Do a pre-processing step especially when WHERE clauses are involved\n",
    "    * Challenge-2: Jargons in the table names or column names\n",
    "    * Solution-2: Provide examples as additional context\n",
    "* Enhancing query\n",
    "    * Challenge: The user query may not be well informed. May require rewording to extract relevant columns from the tables.\n",
    "    * Solution: Ask an LLM to rewrite the query and optimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
